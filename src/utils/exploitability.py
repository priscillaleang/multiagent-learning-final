"""
Exploitability metrics for game theory evaluation.

Exploitability measures how far a strategy profile is from Nash equilibrium.
Lower exploitability indicates a better approximation to Nash equilibrium.
"""

import numpy as np
import pyspiel
from typing import Tuple, Callable, List, Dict
import random


def compute_exact_exploitability(
    strategy_profile: Tuple[Callable, Callable],
    game: pyspiel.Game
) -> float:
    """
    Compute exact exploitability using backward induction (for small games).
    
    Uses full game tree traversal to compute best response values exactly.
    Suitable for small games like Kuhn Poker where the full tree can be enumerated.
    
    Formula: ε = 0.5 * Σ[u_i(BR_i(σ_-i), σ_-i) - u_i(σ)]
    
    Args:
        strategy_profile: Tuple of (policy1, policy2) where each policy is a callable
                         that takes (state, player) and returns action probabilities
        game: OpenSpiel game object (must be 2-player zero-sum)
    
    Returns:
        Scalar exploitability value (non-negative, lower is better)
    
    Raises:
        ValueError: If game is not 2-player or if strategies are invalid
    
    Example:
        >>> import pyspiel
        >>> game = pyspiel.load_game("kuhn_poker")
        >>> 
        >>> def uniform_policy(state, player):
        ...     legal_actions = state.legal_actions()
        ...     probs = np.ones(len(legal_actions)) / len(legal_actions)
        ...     return dict(zip(legal_actions, probs))
        >>> 
        >>> strategies = (uniform_policy, uniform_policy)
        >>> exploitability = compute_exact_exploitability(strategies, game)
        >>> print(f"Exploitability: {exploitability:.4f}")
    """
    if game.num_players() != 2:
        raise ValueError(f"Game must be 2-player, got {game.num_players()} players")
    
    if len(strategy_profile) != 2:
        raise ValueError(f"Strategy profile must have 2 policies, got {len(strategy_profile)}")
    
    policy1, policy2 = strategy_profile
    
    total_exploitability = 0.0
    
    # For each player, compute best response value and current value
    for player in range(2):
        # Get opponent policy
        opponent_policy = policy2 if player == 0 else policy1
        
        # Compute best response value (max value player can achieve against opponent)
        br_value = _compute_best_response_value(
            player, opponent_policy, game
        )
        
        # Compute current expected value under strategy profile
        current_value = _compute_expected_value(
            strategy_profile, player, game
        )
        
        total_exploitability += br_value - current_value
    
    return total_exploitability / 2.0


def _compute_best_response_value(
    player: int,
    opponent_policy: Callable,
    game: pyspiel.Game
) -> float:
    """
    Compute best response value for a player against an opponent policy.
    
    Uses backward induction (dynamic programming) to find optimal actions
    at each information state.
    
    Args:
        player: Player index (0 or 1)
        opponent_policy: Callable that returns action probabilities for opponent
        game: OpenSpiel game object
    
    Returns:
        Expected value under best response
    """
    # Use memoization for efficiency
    memo: Dict[str, float] = {}
    
    def _best_response_recursive(state: pyspiel.State) -> float:
        """Recursive best response computation."""
        if state.is_terminal():
            returns = state.returns()
            return returns[player]
        
        if state.is_chance_node():
            # Average over chance outcomes
            outcomes = state.chance_outcomes()
            value = 0.0
            for action, prob in outcomes:
                new_state = state.clone()
                new_state.apply_action(action)
                value += prob * _best_response_recursive(new_state)
            return value
        
        current_player = state.current_player()
        legal_actions = state.legal_actions()
        
        if current_player == player:
            # Player's turn: choose action that maximizes value
            best_value = float('-inf')
            for action in legal_actions:
                new_state = state.clone()
                new_state.apply_action(action)
                value = _best_response_recursive(new_state)
                best_value = max(best_value, value)
            return best_value
        else:
            # Opponent's turn: use opponent policy
            action_probs = opponent_policy(state, current_player)
            if not isinstance(action_probs, dict):
                # Convert array/list to dict
                action_probs = dict(zip(legal_actions, action_probs))
            
            value = 0.0
            for action in legal_actions:
                prob = action_probs.get(action, 0.0)
                if prob > 0:
                    new_state = state.clone()
                    new_state.apply_action(action)
                    value += prob * _best_response_recursive(new_state)
            return value
    
    initial_state = game.new_initial_state()
    return _best_response_recursive(initial_state)


def _compute_expected_value(
    strategy_profile: Tuple[Callable, Callable],
    player: int,
    game: pyspiel.Game
) -> float:
    """
    Compute expected value for a player under a strategy profile.
    
    Args:
        strategy_profile: Tuple of (policy1, policy2)
        player: Player index (0 or 1)
        game: OpenSpiel game object
    
    Returns:
        Expected value for the player
    """
    policy1, policy2 = strategy_profile
    
    def _expected_value_recursive(state: pyspiel.State) -> float:
        """Recursive expected value computation."""
        if state.is_terminal():
            returns = state.returns()
            return returns[player]
        
        if state.is_chance_node():
            # Average over chance outcomes
            outcomes = state.chance_outcomes()
            value = 0.0
            for action, prob in outcomes:
                new_state = state.clone()
                new_state.apply_action(action)
                value += prob * _expected_value_recursive(new_state)
            return value
        
        current_player = state.current_player()
        legal_actions = state.legal_actions()
        
        # Get policy for current player
        policy = policy1 if current_player == 0 else policy2
        action_probs = policy(state, current_player)
        
        if not isinstance(action_probs, dict):
            # Convert array/list to dict
            action_probs = dict(zip(legal_actions, action_probs))
        
        value = 0.0
        for action in legal_actions:
            prob = action_probs.get(action, 0.0)
            if prob > 0:
                new_state = state.clone()
                new_state.apply_action(action)
                value += prob * _expected_value_recursive(new_state)
        
        return value
    
    initial_state = game.new_initial_state()
    return _expected_value_recursive(initial_state)


def compute_approximate_exploitability(
    strategies: Tuple[Callable, Callable],
    game: pyspiel.Game,
    num_evals: int = 1000
) -> float:
    """
    Compute approximate exploitability using Monte Carlo estimation.
    
    Estimates exploitability by simulating games and computing best response
    values through self-play. Suitable for larger games where exact computation
    is infeasible.
    
    Formula: ε = 0.5 * Σ[u_i(BR_i(σ_-i), σ_-i) - u_i(σ)]
    
    Args:
        strategies: Tuple of (policy1, policy2) where each policy is a callable
                   that takes (state, player) and returns action probabilities
        game: OpenSpiel game object (must be 2-player zero-sum)
        num_evals: Number of Monte Carlo evaluations per player (default: 1000)
    
    Returns:
        Estimated exploitability value (non-negative, lower is better)
    
    Raises:
        ValueError: If game is not 2-player or if strategies are invalid
    
    Example:
        >>> import pyspiel
        >>> import numpy as np
        >>> game = pyspiel.load_game("leduc_poker")
        >>> 
        >>> def random_policy(state, player):
        ...     legal_actions = state.legal_actions()
        ...     probs = np.random.dirichlet(np.ones(len(legal_actions)))
        ...     return dict(zip(legal_actions, probs))
        >>> 
        >>> strategies = (random_policy, random_policy)
        >>> exploitability = compute_approximate_exploitability(strategies, game, num_evals=500)
        >>> print(f"Approximate exploitability: {exploitability:.4f}")
    """
    if game.num_players() != 2:
        raise ValueError(f"Game must be 2-player, got {game.num_players()} players")
    
    if len(strategies) != 2:
        raise ValueError(f"Strategies must have 2 policies, got {len(strategies)}")
    
    if num_evals <= 0:
        raise ValueError(f"num_evals must be positive, got {num_evals}")
    
    policy1, policy2 = strategies
    total_exploitability = 0.0
    
    # For each player, estimate best response value and current value
    for player in range(2):
        # Get opponent policy
        opponent_policy = policy2 if player == 0 else policy1
        
        # Estimate best response value via Monte Carlo
        br_value = _estimate_best_response_value(
            player, opponent_policy, game, num_evals
        )
        
        # Estimate current expected value via Monte Carlo
        current_value = _estimate_expected_value(
            strategies, player, game, num_evals
        )
        
        total_exploitability += br_value - current_value
    
    return total_exploitability / 2.0


def _estimate_best_response_value(
    player: int,
    opponent_policy: Callable,
    game: pyspiel.Game,
    num_evals: int
) -> float:
    """
    Estimate best response value using Monte Carlo simulation.
    
    Plays games where the player uses a greedy best response policy
    (chooses action with highest estimated value) against the opponent.
    
    Args:
        player: Player index (0 or 1)
        opponent_policy: Callable that returns action probabilities for opponent
        game: OpenSpiel game object
        num_evals: Number of evaluation episodes
    
    Returns:
        Estimated expected value under best response
    """
    total_value = 0.0
    
    for _ in range(num_evals):
        state = game.new_initial_state()
        
        # Resolve chance nodes
        while state.is_chance_node():
            outcomes = state.chance_outcomes()
            actions = [o[0] for o in outcomes]
            probs = [o[1] for o in outcomes]
            action = np.random.choice(actions, p=probs)
            state.apply_action(action)
        
        # Play game with best response
        while not state.is_terminal():
            current_player = state.current_player()
            legal_actions = state.legal_actions()
            
            if current_player == player:
                # Best response: try all actions, choose best
                # For efficiency, use a simple heuristic: choose action greedily
                # In practice, this could be improved with value function
                action = _greedy_best_response_action(
                    state, player, opponent_policy, game, legal_actions
                )
            else:
                # Opponent: sample from policy
                action_probs = opponent_policy(state, current_player)
                if isinstance(action_probs, dict):
                    probs = [action_probs.get(a, 0.0) for a in legal_actions]
                else:
                    probs = list(action_probs)
                
                # Normalize probabilities
                probs = np.array(probs)
                probs = probs / (probs.sum() + 1e-10)
                action = np.random.choice(legal_actions, p=probs)
            
            state.apply_action(action)
            
            # Resolve chance nodes
            while state.is_chance_node():
                outcomes = state.chance_outcomes()
                actions = [o[0] for o in outcomes]
                probs = [o[1] for o in outcomes]
                action = np.random.choice(actions, p=probs)
                state.apply_action(action)
        
        # Get final reward
        returns = state.returns()
        total_value += returns[player]
    
    return total_value / num_evals


def _greedy_best_response_action(
    state: pyspiel.State,
    player: int,
    opponent_policy: Callable,
    game: pyspiel.Game,
    legal_actions: List[int]
) -> int:
    """
    Choose action greedily by estimating value of each action.
    
    For each legal action, simulate a few rollouts to estimate value,
    then choose the action with highest estimated value.
    
    Args:
        state: Current game state
        player: Player index
        opponent_policy: Opponent policy
        game: Game object
        legal_actions: List of legal actions
    
    Returns:
        Action with highest estimated value
    """
    if len(legal_actions) == 1:
        return legal_actions[0]
    
    # Simple heuristic: use a small number of rollouts per action
    num_rollouts = min(10, max(1, 100 // len(legal_actions)))
    action_values = []
    
    for action in legal_actions:
        total_value = 0.0
        
        for _ in range(num_rollouts):
            # Clone state and apply action
            test_state = state.clone()
            test_state.apply_action(action)
            
            # Resolve chance nodes
            while test_state.is_chance_node():
                outcomes = test_state.chance_outcomes()
                actions = [o[0] for o in outcomes]
                probs = [o[1] for o in outcomes]
                chance_action = np.random.choice(actions, p=probs)
                test_state.apply_action(chance_action)
            
            # Complete rollout
            value = _rollout_value(test_state, player, opponent_policy, game)
            total_value += value
        
        action_values.append(total_value / num_rollouts)
    
    # Choose action with highest value
    best_idx = np.argmax(action_values)
    return legal_actions[best_idx]


def _rollout_value(
    state: pyspiel.State,
    player: int,
    opponent_policy: Callable,
    game: pyspiel.Game
) -> float:
    """
    Complete a rollout from current state and return value for player.
    
    Args:
        state: Current game state
        player: Player index
        opponent_policy: Opponent policy
        game: Game object
    
    Returns:
        Final reward for player
    """
    current_state = state.clone()
    
    while not current_state.is_terminal():
        current_player = current_state.current_player()
        legal_actions = current_state.legal_actions()
        
        if current_player == player:
            # Player: choose random action (for rollout)
            action = random.choice(legal_actions)
        else:
            # Opponent: sample from policy
            action_probs = opponent_policy(current_state, current_player)
            if isinstance(action_probs, dict):
                probs = [action_probs.get(a, 0.0) for a in legal_actions]
            else:
                probs = list(action_probs)
            
            probs = np.array(probs)
            probs = probs / (probs.sum() + 1e-10)
            action = np.random.choice(legal_actions, p=probs)
        
        current_state.apply_action(action)
        
        # Resolve chance nodes
        while current_state.is_chance_node():
            outcomes = current_state.chance_outcomes()
            actions = [o[0] for o in outcomes]
            probs = [o[1] for o in outcomes]
            chance_action = np.random.choice(actions, p=probs)
            current_state.apply_action(chance_action)
    
    returns = current_state.returns()
    return returns[player]


def _estimate_expected_value(
    strategies: Tuple[Callable, Callable],
    player: int,
    game: pyspiel.Game,
    num_evals: int
) -> float:
    """
    Estimate expected value using Monte Carlo simulation.
    
    Args:
        strategies: Tuple of (policy1, policy2)
        player: Player index
        game: Game object
        num_evals: Number of evaluation episodes
    
    Returns:
        Estimated expected value
    """
    policy1, policy2 = strategies
    total_value = 0.0
    
    for _ in range(num_evals):
        state = game.new_initial_state()
        
        # Resolve chance nodes
        while state.is_chance_node():
            outcomes = state.chance_outcomes()
            actions = [o[0] for o in outcomes]
            probs = [o[1] for o in outcomes]
            action = np.random.choice(actions, p=probs)
            state.apply_action(action)
        
        # Play game with both policies
        while not state.is_terminal():
            current_player = state.current_player()
            legal_actions = state.legal_actions()
            
            # Get policy for current player
            policy = policy1 if current_player == 0 else policy2
            action_probs = policy(state, current_player)
            
            if isinstance(action_probs, dict):
                probs = [action_probs.get(a, 0.0) for a in legal_actions]
            else:
                probs = list(action_probs)
            
            # Normalize and sample
            probs = np.array(probs)
            probs = probs / (probs.sum() + 1e-10)
            action = np.random.choice(legal_actions, p=probs)
            
            state.apply_action(action)
            
            # Resolve chance nodes
            while state.is_chance_node():
                outcomes = state.chance_outcomes()
                actions = [o[0] for o in outcomes]
                probs = [o[1] for o in outcomes]
                chance_action = np.random.choice(actions, p=probs)
                state.apply_action(chance_action)
        
        # Get final reward
        returns = state.returns()
        total_value += returns[player]
    
    return total_value / num_evals

